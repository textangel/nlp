{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Masked Word Prediction\n",
    "Source: https://huggingface.co/transformers/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_weights = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(pre_trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[CLS] 你 会 说 西 班 牙 语 吗 ？ [SEP] 对 ， 我 是 在 [MASK] [MASK] 的 首 都 利 马 长 大 的  。 [SEP]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Get Masked Index\n",
    "masked_index = [i for i, x in enumerate(tokenized_text) if x == \"[MASK]\"]\n",
    "\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "next_sent_start_ix = tokenized_text.index('[SEP]')\n",
    "segments_ids = [0] * len(tokenized_text)\n",
    "segments_ids[next_sent_start_ix+1:] = [1] * len(segments_ids[next_sent_start_ix+1:])\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 17]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BertModel to encode our inputs in hidden-states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is unnecessary for the next cell\n",
    "model = BertModel.from_pretrained(pre_trained_weights)\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# tokens_tensor = tokens_tensor.to('cuda')\n",
    "# segments_tensors = segments_tensors.to('cuda')\n",
    "# model.to('cuda')\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    # Transformers models always output tuples.\n",
    "    # See the models docstrings for the detail of all the outputs\n",
    "    # In our case, the first element is the hidden state of the last layer of the Bert model\n",
    "    encoded_layers = outputs[0]\n",
    "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
    "assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above cell is unnecessary for this part.\n",
    "top_k = 10\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(pre_trained_weights)\n",
    "model.eval()\n",
    "# If you have a GPU, put everything on cuda here, see above cell.\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "    predictions_of_mask = predictions[0, masked_index]\n",
    "    probs_for_mask = F.softmax(predictions_of_mask, dim=1)\n",
    "    \n",
    "# confirm we were able to predict the output\n",
    "predicted_indices = torch.topk(probs_for_mask, k=top_k).indices\n",
    "predicted_probs = torch.topk(probs_for_mask, k=top_k).values.numpy()\n",
    "predicted_tokens = [tokenizer.convert_ids_to_tokens(tok) for tok in predicted_indices]\n",
    "[dict(zip(predicted_tokens[i], predicted_probs[i])) for i in range(len(predicted_tokens))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT for Chinese Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_weights = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(pre_trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, masklen=1):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    mask_ix = tokenized_text.index('[MASK]')\n",
    "    tokenized_text_cp = tokenized_text.copy()\n",
    "    tokenized_text_cp[mask_ix:mask_ix] = ['[MASK]'] * masklen\n",
    "    return tokenized_text_cp\n",
    "\n",
    "\n",
    "def evaluate_one_word(tokenized_text, topk=4):\n",
    "    \"\"\"\n",
    "    tokenized_text: tokenized_text\n",
    "    \"\"\"\n",
    "    print(f\"Number of [MASK]: {tokenized_text.count('[MASK]')}\")\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    masked_index = [ix for ix, x in enumerate(tokenized_text) if x == \"[MASK]\"]\n",
    "\n",
    "    segments_ids = [0] * len(tokenized_text)\n",
    "    next_sent_start_ix = tokenized_text.index('[SEP]')\n",
    "    segments_ids[next_sent_start_ix + 1:] = [1] * len(segments_ids[next_sent_start_ix + 1:])\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    model = BertForMaskedLM.from_pretrained(pre_trained_weights)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "        predictions = outputs[0][0, masked_index]\n",
    "        probs_for_mask = F.softmax(predictions, dim=1)\n",
    "\n",
    "    # confirm we were able to predict the output\n",
    "    predicted_indices = torch.topk(probs_for_mask, k=topk).indices\n",
    "    predicted_probs = -1 * torch.topk(probs_for_mask, k=topk).values.numpy()\n",
    "    predicted_tokens = [tokenizer.convert_ids_to_tokens(tok) for tok in predicted_indices]\n",
    "    return dict(zip(predicted_probs[0], predicted_tokens[0]))\n",
    "\n",
    "def get_num_masks(tokenized_text):\n",
    "    return tokenized_text.count('[MASK]')\n",
    "\n",
    "\n",
    "def update_probs(prb, new_prb):\n",
    "    return -1 * prb * new_prb\n",
    "\n",
    "\n",
    "def stepwise_beam_search(tokenized_text):\n",
    "    num_masks = get_num_masks(tokenized_text)\n",
    "    beam_size = 20\n",
    "    eval_count = beam_size\n",
    "    best_of_len = {0: [(-1, [])]}\n",
    "    for length in range(1, num_masks + 1):\n",
    "        print(\"For Loop no: \", length)\n",
    "        for prb0, str0 in best_of_len[length - 1]:\n",
    "            print('best_of_len: ', best_of_len)\n",
    "            tokenized_text_cp = tokenized_text.copy()\n",
    "            mask_ix_start = tokenized_text_cp.index('[MASK]')\n",
    "            tokenized_text_cp[mask_ix_start:mask_ix_start + length - 1] = str0\n",
    "            print(\"Text before processing \", tokenized_text_cp)\n",
    "            res = evaluate_one_word(tokenized_text_cp, topk=eval_count)\n",
    "            updated_res = [(update_probs(prb, prb0), str0 + [char]) for prb, char in res.items()]\n",
    "            if length not in best_of_len:\n",
    "                best_of_len[length] = []\n",
    "            best_of_len[length] += updated_res\n",
    "            best_of_len[length] = sorted(best_of_len[length], key=lambda x: x[0])\n",
    "        best_of_len[length] = best_of_len[length][:beam_size]\n",
    "    return best_of_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes an input text of the form `\"[CLS] <text1> [SEP] <text2> [SEP]\"`, where `\"<text1>\", \"<text2>\"` are space-separated chinese characters, and exactly one of the characters is `[MASK]`. \n",
    "We replace `[MASK]` with `mask_len` number of `[MASK]` characters, and then perform beam search to find the optimal n-character sequence to fill in that blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_len = 3n \n",
    "text = \"[CLS] 把 台 上 几 个 原  本 羞 却 [MASK] 的 男 孩 们 炒 成 了 热 门 的 幕 间 演 出 乐 队 。 [SEP] 他 们 就 这 样 学 会 了 如 何 抓 住 持 续 增 长 的 听 众 。 [SEP]\"\n",
    "tokenized_text = tokenize_text(text, masklen=mask_len)\n",
    "result = stepwise_beam_search(tokenized_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Predict Just one\n",
    "# predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "# predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
